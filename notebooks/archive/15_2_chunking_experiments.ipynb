{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial_id",
   "metadata": {},
   "source": "# Chunking Experiments with MLflow Tracking\n\nThis notebook explores different chunking strategies for RAG:\n- Multiple chunk sizes (128, 256, 512, 1024 tokens)\n- Multiple overlap values (0, 25, 50 tokens)\n- Multiple embedders\n- Token-based chunking using each embedder's tokenizer\n\nAll experiments are tracked in MLflow for comparison."
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:28:22.610761Z",
     "start_time": "2025-11-03T07:28:19.966599Z"
    }
   },
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import gc\n",
    "from time import time\n",
    "from datasets import load_from_disk, disable_caching, Dataset, load_dataset\n",
    "from rag.config import PROJECT_ROOT\n",
    "from rag.tracking import ExperimentTracker\n",
    "from rag.utils import embed_dataset, get_metrics\n",
    "from rag.embeddings import LocalEmbedder\n",
    "from rag.config import settings\n",
    "from rag.ingestion.chunker import RecursiveChunker, SemanticChunker\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "# Suppress LangChain logging messages\n",
    "logging.getLogger('langchain_text_splitters').setLevel(logging.ERROR)\n",
    "\n",
    "# Suppress specific spaCy warnings\n",
    "warnings.filterwarnings('ignore', message='.*Model.*was trained with spaCy.*')\n",
    "warnings.filterwarnings('ignore', message='.*rule-based lemmatizer did not find POS annotation.*')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ergot/projects/rag/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "load_data",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:28:28.852374Z",
     "start_time": "2025-11-03T07:28:22.613407Z"
    }
   },
   "source": [
    "# Load datasets\n",
    "doc_ds = load_dataset(\"rag-datasets/rag-mini-bioasq\", \"text-corpus\", split=\"passages\")\n",
    "doc_ds = doc_ds.filter(lambda row: row['passage'] != 'nan')\n",
    "query_ds = load_dataset(\"rag-datasets/rag-mini-bioasq\", \"question-answer-passages\", split=\"test\")\n",
    "\n",
    "# Precompute\n",
    "doc_id_to_text = doc_ds.select_columns(['id', 'passage']).to_pandas().set_index('id')['passage'].to_dict()\n",
    "queries = np.array(query_ds['question'])\n",
    "qrels = [np.array(eval(gold)) for gold in query_ds['relevant_passage_ids']]\n",
    "qrels_counts = [len(s) for s in qrels]\n",
    "\n",
    "disable_caching()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "experiment_config",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:28:28.911521Z",
     "start_time": "2025-11-03T07:28:28.897978Z"
    }
   },
   "source": [
    "# Initialize tracker\n",
    "tracker = ExperimentTracker('chunking-semantic-bioasq')\n",
    "\n",
    "# Experiment configuration\n",
    "embedder_models = [\n",
    "    # \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    # \"sentence-transformers/all-MiniLM-L12-v2\",\n",
    "    \"BAAI/bge-small-en-v1.5\",\n",
    "    # \"BAAI/bge-base-en-v1.5\",\n",
    "    # \"BAAI/bge-large-en-v1.5\",\n",
    "]\n",
    "\n",
    "# chunkers = ['recursive', 'semantic']\n",
    "chunkers = ['semantic']\n",
    "chunk_sizes = [64, 128]  # None = no chunking, others in tokens\n",
    "chunk_overlaps = [0, 50, 100]  # in tokens\n",
    "chunk_thresholds = [0.1, 0.2, 0.3]\n",
    "\n",
    "faiss_metric = 'IP'\n",
    "rerank_model = None\n",
    "\n",
    "\n",
    "def deduplicate_retrieved_docs(retrieved_ids_all, k):\n",
    "    \"\"\"\n",
    "    Deduplicate document IDs per query, keeping only first occurrence.\n",
    "    \n",
    "    For chunked retrieval, multiple chunks from same document may be retrieved.\n",
    "    This keeps only the highest-ranked occurrence of each unique document.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_ids_all: (n_queries, n_retrieved) array of document IDs\n",
    "        k: Number of unique documents to keep per query\n",
    "    \n",
    "    Returns:\n",
    "        (n_queries, k) array of unique document IDs\n",
    "    \"\"\"\n",
    "    deduped = []\n",
    "    for query_results in retrieved_ids_all:\n",
    "        seen = set()\n",
    "        unique_docs = []\n",
    "        for doc_id in query_results:\n",
    "            if doc_id not in seen:\n",
    "                unique_docs.append(doc_id)\n",
    "                seen.add(doc_id)\n",
    "            if len(unique_docs) == k:\n",
    "                break\n",
    "        # Pad if needed\n",
    "        while len(unique_docs) < k:\n",
    "            unique_docs.append(0)  # Padding with 0 (won't match any real doc)\n",
    "        deduped.append(unique_docs)\n",
    "    return np.array(deduped)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-03 08:28:28] [rag.tracking] [INFO] Tracking to: http://localhost:5000\n",
      "[2025-11-03 08:28:28] [rag.tracking] [INFO] Experiment: chunking-semantic-bioasq\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:28:28.946538Z",
     "start_time": "2025-11-03T07:28:28.943418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "\n",
    "list(itertools.product(chunkers, chunk_sizes, chunk_overlaps))"
   ],
   "id": "615e8d631c8f4c11",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('semantic', 64, 0),\n",
       " ('semantic', 64, 50),\n",
       " ('semantic', 64, 100),\n",
       " ('semantic', 128, 0),\n",
       " ('semantic', 128, 50),\n",
       " ('semantic', 128, 100)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "run_experiments",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:12:15.448799Z",
     "start_time": "2025-11-03T07:28:28.987174Z"
    }
   },
   "source": [
    "# Run experiments\n",
    "for embedder_name in embedder_models:\n",
    "    embedder_name_short = embedder_name.split('/')[-1]\n",
    "    \n",
    "    for chunker_type, chunk_size, chunk_overlap in itertools.product(chunkers, chunk_sizes, chunk_overlaps):\n",
    "            # skip unreal splits\n",
    "            if chunk_size <= chunk_overlap:\n",
    "                continue\n",
    "\n",
    "            thresholds = chunk_thresholds if chunker_type == 'semantic' else [None]\n",
    "\n",
    "            for threshold in thresholds:\n",
    "\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"Testing: {embedder_name_short} | chunker={chunker_type} | chunk_size={chunk_size} | overlap={chunk_overlap} | threshold={threshold}\")\n",
    "                print(f\"{'='*80}\")\n",
    "\n",
    "                try:\n",
    "                    # Create chunked dataset using token-based chunking\n",
    "                    if chunker_type == 'recursive':\n",
    "                        chunker = RecursiveChunker(\n",
    "                            chunk_size=chunk_size,\n",
    "                            chunk_overlap=chunk_overlap,\n",
    "                            embedder_model=embedder_name,\n",
    "                        )\n",
    "                    elif chunker_type == 'semantic':\n",
    "                        chunker = SemanticChunker(\n",
    "                            chunk_size=chunk_size,\n",
    "                            chunk_overlap=chunk_overlap,\n",
    "                            embedder_model=embedder_name,\n",
    "                            threshold=threshold,\n",
    "                        )\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown chunker type {chunker_type}\")\n",
    "\n",
    "                    current_doc_ds = chunker.chunk_dataset(doc_ds, text_col='passage', id_col='id')\n",
    "                    # Rename columns to match expected format\n",
    "                    current_doc_ds = current_doc_ds.rename_column('doc_id', 'parent_id')\n",
    "                    current_doc_ds = current_doc_ds.rename_column('text', 'passage')\n",
    "                    print(f\"Created {len(current_doc_ds)} chunks from {len(doc_ds)} documents\")\n",
    "\n",
    "                    # Initialize embedder\n",
    "                    embedder = LocalEmbedder(embedder_name, device=\"cuda\")\n",
    "\n",
    "                    # Embed documents and queries\n",
    "                    start_time = time()\n",
    "                    current_doc_ds = embed_dataset(current_doc_ds, embedder, column=\"passage\")\n",
    "                    current_query_ds = embed_dataset(query_ds, embedder, column=\"question\")\n",
    "                    elapsed_time = time() - start_time\n",
    "\n",
    "                    # Build FAISS index\n",
    "                    current_doc_ds.add_faiss_index(\n",
    "                        column='embedding',\n",
    "                        string_factory='Flat',\n",
    "                        metric_type=faiss.METRIC_INNER_PRODUCT,\n",
    "                        batch_size=128,\n",
    "                    )\n",
    "\n",
    "                    # Retrieve top-100 chunks/documents\n",
    "                    retrieve_k = 100\n",
    "                    res = current_doc_ds.get_index('embedding').search_batch(\n",
    "                        np.array(current_query_ds['embedding']),\n",
    "                        k=retrieve_k\n",
    "                    )\n",
    "\n",
    "                    index_to_doc_id = np.array(current_doc_ds['parent_id'])\n",
    "\n",
    "                    retrieved_ids_all = index_to_doc_id[res.total_indices]\n",
    "\n",
    "                    # Deduplicate documents (keeping top-100 unique docs)\n",
    "                    retrieved_ids_all = deduplicate_retrieved_docs(retrieved_ids_all, retrieve_k)\n",
    "\n",
    "                    # Calculate metrics at different k values\n",
    "                    metrics = {}\n",
    "                    for k in [1, 3, 5, 10]:\n",
    "                        retrieved_ids = retrieved_ids_all[:, :k]\n",
    "                        metrics = {\n",
    "                            **metrics,\n",
    "                            **get_metrics(retrieved_ids, query_ds, k),\n",
    "                        }\n",
    "\n",
    "                    metrics = {\n",
    "                        **{k: round(v, 4) for k, v in metrics.items()},\n",
    "                        \"elapsed_time\": round(elapsed_time, 1),\n",
    "                        \"num_chunks\": len(current_doc_ds),\n",
    "                    }\n",
    "\n",
    "                    # Parameters to track\n",
    "                    params = {\n",
    "                        'embed_model': embedder_name,\n",
    "                        'rerank_model': rerank_model,\n",
    "                        'chunker': chunker_type,\n",
    "                        'chunk_size': chunk_size if chunk_size is not None else 'none',\n",
    "                        'chunk_overlap': chunk_overlap if chunk_size is not None else 'none',\n",
    "                        'chunk_threshold': threshold,\n",
    "                        'faiss_metric': faiss_metric,\n",
    "                    }\n",
    "\n",
    "                    run_name = f\"{embedder_name_short}_{chunker_type}_cs{chunk_size}_ov{chunk_overlap}_thr{threshold}\"\n",
    "\n",
    "                    # Tags for filtering\n",
    "                    tags = {\n",
    "                        'experiment_type': 'chunking',\n",
    "                        'phase': 'exploration',\n",
    "                        'dataset': 'bioasq-mini',\n",
    "                        'embedder': embedder_name_short,\n",
    "                        'chunker': chunker_type,\n",
    "                    }\n",
    "\n",
    "                    # Log to MLflow\n",
    "                    with tracker.start_run(run_name=run_name, tags=tags):\n",
    "                        tracker.log_params(params)\n",
    "                        tracker.log_metrics(metrics)\n",
    "\n",
    "                    print(f\"\\nResults:\")\n",
    "                    print(f\"  P@10: {metrics.get('P@10', 0):.4f}\")\n",
    "                    print(f\"  R@10: {metrics.get('R@10', 0):.4f}\")\n",
    "                    print(f\"  MRR@10: {metrics.get('MRR@10', 0):.4f}\")\n",
    "                    print(f\"  nDCG@10: {metrics.get('nDCG@10', 0):.4f}\")\n",
    "                    print(f\"  Time: {elapsed_time:.1f}s\")\n",
    "\n",
    "                    # Cleanup\n",
    "                    current_doc_ds.drop_index('embedding')\n",
    "                    del embedder\n",
    "                    del current_doc_ds\n",
    "                    del current_query_ds\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nFailed: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All experiments completed!\")\n",
    "print(\"=\"*80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing: bge-small-en-v1.5 | chunker=semantic | chunk_size=64 | overlap=0 | threshold=0.1\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking:  22%|â–ˆâ–ˆâ–       | 6242/28001 [01:50<06:04, 59.67it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28001/28001 [08:53<00:00, 52.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 196671 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196671/196671 [02:00<00:00, 1631.01 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4719/4719 [00:02<00:00, 2118.39 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1537/1537 [00:00<00:00, 4251.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bge-small-en-v1.5_semantic_cs64_ov0_thr0.1 at: http://localhost:5000/#/experiments/251462098066741785/runs/1d382fd1e0d144119f8b104b44597f5d\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/251462098066741785\n",
      "\n",
      "Results:\n",
      "  P@10: 0.3535\n",
      "  R@10: 0.4749\n",
      "  MRR@10: 0.7542\n",
      "  nDCG@10: 0.5851\n",
      "  Time: 139.3s\n",
      "\n",
      "================================================================================\n",
      "Testing: bge-small-en-v1.5 | chunker=semantic | chunk_size=64 | overlap=0 | threshold=0.2\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking:  22%|â–ˆâ–ˆâ–       | 6236/28001 [01:52<06:02, 59.97it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28001/28001 [08:59<00:00, 51.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 113944 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113944/113944 [01:43<00:00, 1101.06 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4719/4719 [00:02<00:00, 2109.11 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 891/891 [00:00<00:00, 4649.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bge-small-en-v1.5_semantic_cs64_ov0_thr0.2 at: http://localhost:5000/#/experiments/251462098066741785/runs/b5c4dd1f57b047af8f4532cbd103665b\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/251462098066741785\n",
      "\n",
      "Results:\n",
      "  P@10: 0.3437\n",
      "  R@10: 0.4601\n",
      "  MRR@10: 0.7439\n",
      "  nDCG@10: 0.5702\n",
      "  Time: 122.2s\n",
      "\n",
      "================================================================================\n",
      "Testing: bge-small-en-v1.5 | chunker=semantic | chunk_size=64 | overlap=0 | threshold=0.3\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking:  22%|â–ˆâ–ˆâ–       | 6238/28001 [01:52<06:00, 60.44it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28001/28001 [08:59<00:00, 51.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 44317 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44317/44317 [00:55<00:00, 803.50 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4719/4719 [00:02<00:00, 2143.77 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 347/347 [00:00<00:00, 4370.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bge-small-en-v1.5_semantic_cs64_ov0_thr0.3 at: http://localhost:5000/#/experiments/251462098066741785/runs/e5d16e9c0daf4fc78a7a479a74254631\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/251462098066741785\n",
      "\n",
      "Results:\n",
      "  P@10: 0.3377\n",
      "  R@10: 0.4476\n",
      "  MRR@10: 0.7345\n",
      "  nDCG@10: 0.5598\n",
      "  Time: 73.7s\n",
      "\n",
      "================================================================================\n",
      "Testing: bge-small-en-v1.5 | chunker=semantic | chunk_size=64 | overlap=50 | threshold=0.1\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking:  22%|â–ˆâ–ˆâ–       | 6241/28001 [01:53<06:11, 58.55it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28001/28001 [09:02<00:00, 51.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 197664 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197664/197664 [02:07<00:00, 1555.31 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4719/4719 [00:02<00:00, 2154.52 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1545/1545 [00:00<00:00, 4319.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bge-small-en-v1.5_semantic_cs64_ov50_thr0.1 at: http://localhost:5000/#/experiments/251462098066741785/runs/755fba3627044994b8a0c4b002ae8c08\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/251462098066741785\n",
      "\n",
      "Results:\n",
      "  P@10: 0.3538\n",
      "  R@10: 0.4758\n",
      "  MRR@10: 0.7562\n",
      "  nDCG@10: 0.5870\n",
      "  Time: 145.6s\n",
      "\n",
      "================================================================================\n",
      "Testing: bge-small-en-v1.5 | chunker=semantic | chunk_size=64 | overlap=50 | threshold=0.2\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking:  22%|â–ˆâ–ˆâ–       | 6238/28001 [01:53<06:14, 58.18it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28001/28001 [09:03<00:00, 51.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 101645 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101645/101645 [01:42<00:00, 992.79 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4719/4719 [00:02<00:00, 2124.44 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 795/795 [00:00<00:00, 4349.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bge-small-en-v1.5_semantic_cs64_ov50_thr0.2 at: http://localhost:5000/#/experiments/251462098066741785/runs/70671f9923144d7ca8ecbc802476861a\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/251462098066741785\n",
      "\n",
      "Results:\n",
      "  P@10: 0.3426\n",
      "  R@10: 0.4574\n",
      "  MRR@10: 0.7437\n",
      "  nDCG@10: 0.5686\n",
      "  Time: 121.3s\n",
      "\n",
      "================================================================================\n",
      "Testing: bge-small-en-v1.5 | chunker=semantic | chunk_size=64 | overlap=50 | threshold=0.3\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0567013c-9680-4d26-a1eb-b67321272beb)')' thrown while requesting HEAD https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Chunking:  22%|â–ˆâ–ˆâ–       | 6241/28001 [01:53<06:17, 57.62it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28001/28001 [09:03<00:00, 51.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 39951 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39951/39951 [00:53<00:00, 748.88 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4719/4719 [00:02<00:00, 2080.17 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:00<00:00, 4178.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bge-small-en-v1.5_semantic_cs64_ov50_thr0.3 at: http://localhost:5000/#/experiments/251462098066741785/runs/de7dd7371d3741848cf42242155a366f\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/251462098066741785\n",
      "\n",
      "Results:\n",
      "  P@10: 0.3360\n",
      "  R@10: 0.4445\n",
      "  MRR@10: 0.7329\n",
      "  nDCG@10: 0.5565\n",
      "  Time: 72.0s\n",
      "\n",
      "================================================================================\n",
      "Testing: bge-small-en-v1.5 | chunker=semantic | chunk_size=128 | overlap=0 | threshold=0.1\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking:  22%|â–ˆâ–ˆâ–       | 6238/28001 [01:52<05:59, 60.56it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28001/28001 [08:51<00:00, 52.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 97574 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97574/97574 [01:16<00:00, 1278.77 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4719/4719 [00:02<00:00, 2113.43 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 763/763 [00:00<00:00, 4437.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bge-small-en-v1.5_semantic_cs128_ov0_thr0.1 at: http://localhost:5000/#/experiments/251462098066741785/runs/694eafc5d5ea4f2da6c82752b89f917e\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/251462098066741785\n",
      "\n",
      "Results:\n",
      "  P@10: 0.3546\n",
      "  R@10: 0.4729\n",
      "  MRR@10: 0.7565\n",
      "  nDCG@10: 0.5878\n",
      "  Time: 94.9s\n",
      "\n",
      "================================================================================\n",
      "Testing: bge-small-en-v1.5 | chunker=semantic | chunk_size=128 | overlap=0 | threshold=0.2\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking:  22%|â–ˆâ–ˆâ–       | 6242/28001 [01:50<06:03, 59.82it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28001/28001 [08:49<00:00, 52.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 48287 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48287/48287 [00:57<00:00, 840.16 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4719/4719 [00:02<00:00, 2121.32 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 378/378 [00:00<00:00, 4721.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bge-small-en-v1.5_semantic_cs128_ov0_thr0.2 at: http://localhost:5000/#/experiments/251462098066741785/runs/b96fb788a4924a73947034a14b77d6da\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/251462098066741785\n",
      "\n",
      "Results:\n",
      "  P@10: 0.3428\n",
      "  R@10: 0.4555\n",
      "  MRR@10: 0.7375\n",
      "  nDCG@10: 0.5668\n",
      "  Time: 76.0s\n",
      "\n",
      "================================================================================\n",
      "Testing: bge-small-en-v1.5 | chunker=semantic | chunk_size=128 | overlap=0 | threshold=0.3\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking:  22%|â–ˆâ–ˆâ–       | 6238/28001 [01:52<05:56, 61.06it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28001/28001 [08:59<00:00, 51.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 30899 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30899/30899 [00:40<00:00, 753.88 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4719/4719 [00:02<00:00, 2145.00 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 242/242 [00:00<00:00, 4972.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bge-small-en-v1.5_semantic_cs128_ov0_thr0.3 at: http://localhost:5000/#/experiments/251462098066741785/runs/e779858d1a854680ab3855a24a2a4ac0\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/251462098066741785\n",
      "\n",
      "Results:\n",
      "  P@10: 0.3391\n",
      "  R@10: 0.4486\n",
      "  MRR@10: 0.7339\n",
      "  nDCG@10: 0.5609\n",
      "  Time: 59.6s\n",
      "\n",
      "================================================================================\n",
      "Testing: bge-small-en-v1.5 | chunker=semantic | chunk_size=128 | overlap=50 | threshold=0.1\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking:  22%|â–ˆâ–ˆâ–       | 6236/28001 [01:51<06:21, 57.11it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28001/28001 [08:56<00:00, 52.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 96063 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96063/96063 [01:29<00:00, 1079.20 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4719/4719 [00:02<00:00, 2181.71 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 751/751 [00:00<00:00, 4757.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bge-small-en-v1.5_semantic_cs128_ov50_thr0.1 at: http://localhost:5000/#/experiments/251462098066741785/runs/7c1468a3c1154b2e9f8a870ebb12fcd1\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/251462098066741785\n",
      "\n",
      "Results:\n",
      "  P@10: 0.3530\n",
      "  R@10: 0.4708\n",
      "  MRR@10: 0.7533\n",
      "  nDCG@10: 0.5846\n",
      "  Time: 107.2s\n",
      "\n",
      "================================================================================\n",
      "Testing: bge-small-en-v1.5 | chunker=semantic | chunk_size=128 | overlap=50 | threshold=0.2\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking:  22%|â–ˆâ–ˆâ–       | 6238/28001 [01:52<05:57, 60.90it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28001/28001 [08:59<00:00, 51.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 38349 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38349/38349 [00:52<00:00, 727.13 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4719/4719 [00:02<00:00, 2146.53 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 4252.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bge-small-en-v1.5_semantic_cs128_ov50_thr0.2 at: http://localhost:5000/#/experiments/251462098066741785/runs/e9e8aca134a245448fa0a5e8307ca60d\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/251462098066741785\n",
      "\n",
      "Results:\n",
      "  P@10: 0.3366\n",
      "  R@10: 0.4464\n",
      "  MRR@10: 0.7321\n",
      "  nDCG@10: 0.5573\n",
      "  Time: 71.4s\n",
      "\n",
      "================================================================================\n",
      "Testing: bge-small-en-v1.5 | chunker=semantic | chunk_size=128 | overlap=50 | threshold=0.3\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking:  22%|â–ˆâ–ˆâ–       | 6238/28001 [01:51<05:56, 60.97it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28001/28001 [08:59<00:00, 51.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 28939 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28939/28939 [00:41<00:00, 700.00 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4719/4719 [00:02<00:00, 2145.96 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 227/227 [00:00<00:00, 4931.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bge-small-en-v1.5_semantic_cs128_ov50_thr0.3 at: http://localhost:5000/#/experiments/251462098066741785/runs/772af2a2614446b8a13b320ee7b335d6\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/251462098066741785\n",
      "\n",
      "Results:\n",
      "  P@10: 0.3354\n",
      "  R@10: 0.4429\n",
      "  MRR@10: 0.7319\n",
      "  nDCG@10: 0.5547\n",
      "  Time: 59.9s\n",
      "\n",
      "================================================================================\n",
      "Testing: bge-small-en-v1.5 | chunker=semantic | chunk_size=128 | overlap=100 | threshold=0.1\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking:  22%|â–ˆâ–ˆâ–       | 6239/28001 [01:55<06:17, 57.63it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28001/28001 [09:11<00:00, 50.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 75756 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75756/75756 [01:36<00:00, 786.88 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4719/4719 [00:02<00:00, 2080.55 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 592/592 [00:00<00:00, 4222.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bge-small-en-v1.5_semantic_cs128_ov100_thr0.1 at: http://localhost:5000/#/experiments/251462098066741785/runs/1c1fb572258147989645627e8b61f87c\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/251462098066741785\n",
      "\n",
      "Results:\n",
      "  P@10: 0.3473\n",
      "  R@10: 0.4627\n",
      "  MRR@10: 0.7469\n",
      "  nDCG@10: 0.5752\n",
      "  Time: 115.0s\n",
      "\n",
      "================================================================================\n",
      "Testing: bge-small-en-v1.5 | chunker=semantic | chunk_size=128 | overlap=100 | threshold=0.2\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking:  22%|â–ˆâ–ˆâ–       | 6239/28001 [01:55<06:03, 59.80it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28001/28001 [09:11<00:00, 50.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 32519 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32519/32519 [00:48<00:00, 671.86 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4719/4719 [00:02<00:00, 2090.73 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [00:00<00:00, 4935.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bge-small-en-v1.5_semantic_cs128_ov100_thr0.2 at: http://localhost:5000/#/experiments/251462098066741785/runs/67b804e6e80a4c73b0abc81d92f4ad1b\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/251462098066741785\n",
      "\n",
      "Results:\n",
      "  P@10: 0.3320\n",
      "  R@10: 0.4415\n",
      "  MRR@10: 0.7272\n",
      "  nDCG@10: 0.5503\n",
      "  Time: 66.8s\n",
      "\n",
      "================================================================================\n",
      "Testing: bge-small-en-v1.5 | chunker=semantic | chunk_size=128 | overlap=100 | threshold=0.3\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking:  22%|â–ˆâ–ˆâ–       | 6238/28001 [01:54<06:06, 59.43it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28001/28001 [09:09<00:00, 50.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 28338 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28338/28338 [00:42<00:00, 668.27 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4719/4719 [00:02<00:00, 2130.52 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 222/222 [00:00<00:00, 4924.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bge-small-en-v1.5_semantic_cs128_ov100_thr0.3 at: http://localhost:5000/#/experiments/251462098066741785/runs/dd52765b57bc472586958b3bd9ea8f84\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/251462098066741785\n",
      "\n",
      "Results:\n",
      "  P@10: 0.3313\n",
      "  R@10: 0.4371\n",
      "  MRR@10: 0.7267\n",
      "  nDCG@10: 0.5484\n",
      "  Time: 60.6s\n",
      "\n",
      "================================================================================\n",
      "All experiments completed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "analysis",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T10:12:15.556748Z",
     "start_time": "2025-11-03T10:12:15.554973Z"
    }
   },
   "source": [
    "# Optional: Quick analysis of results\n",
    "print(\"\\nView all results in MLflow UI at: http://localhost:5000\")\n",
    "print(\"\\nKey questions to explore:\")\n",
    "print(\"1. Does chunking improve retrieval performance?\")\n",
    "print(\"2. What is the optimal chunk size for each embedder?\")\n",
    "print(\"3. Does overlap help? What's the optimal overlap?\")\n",
    "print(\"4. How does chunking affect embedding time?\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "View all results in MLflow UI at: http://localhost:5000\n",
      "\n",
      "Key questions to explore:\n",
      "1. Does chunking improve retrieval performance?\n",
      "2. What is the optimal chunk size for each embedder?\n",
      "3. Does overlap help? What's the optimal overlap?\n",
      "4. How does chunking affect embedding time?\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1812c026722245fd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
