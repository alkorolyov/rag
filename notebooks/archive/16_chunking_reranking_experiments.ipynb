{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "# Chunking + Reranking Experiments\n",
    "\n",
    "This notebook combines chunking strategies with reranking to find the optimal RAG pipeline:\n",
    "\n",
    "## Experimental Design:\n",
    "- **Chunkers**: Recursive, Semantic\n",
    "- **Chunk sizes**: 64, 128 tokens\n",
    "- **Overlaps**: 0, 50 tokens\n",
    "- **Semantic thresholds**: 0.1, 0.2, 0.3\n",
    "- **Embedders**: BGE-small, BGE-base\n",
    "- **Rerankers**: MiniLM-L6, MiniLM-L12, BGE-reranker\n",
    "\n",
    "All experiments tracked in MLflow for comparison."
   ]
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T05:13:23.798545Z",
     "start_time": "2025-11-04T05:13:21.380192Z"
    }
   },
   "source": [
    "import warnings\n",
    "import logging\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import gc\n",
    "from time import time\n",
    "from datasets import load_dataset, disable_caching\n",
    "from sentence_transformers import CrossEncoder\n",
    "from FlagEmbedding import FlagReranker\n",
    "\n",
    "from rag.tracking import ExperimentTracker\n",
    "from rag.utils import embed_dataset, get_metrics\n",
    "from rag.embeddings import LocalEmbedder\n",
    "from rag.ingestion.chunker import RecursiveChunker, SemanticChunker\n",
    "\n",
    "# Suppress warnings and logs\n",
    "warnings.filterwarnings('ignore', message='.*Model.*was trained with spaCy.*')\n",
    "warnings.filterwarnings('ignore', message='.*rule-based lemmatizer did not find POS annotation.*')\n",
    "logging.getLogger('langchain_text_splitters').setLevel(logging.ERROR)\n",
    "\n",
    "print(\"✓ Setup complete\")"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ergot/projects/rag/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "id": "load_data",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T05:13:29.562737Z",
     "start_time": "2025-11-04T05:13:24.379867Z"
    }
   },
   "source": [
    "# Load datasets\n",
    "print(\"Loading BioASQ dataset...\")\n",
    "doc_ds = load_dataset(\"rag-datasets/rag-mini-bioasq\", \"text-corpus\", split=\"passages\")\n",
    "doc_ds = doc_ds.filter(lambda row: row['passage'] != 'nan')\n",
    "query_ds = load_dataset(\"rag-datasets/rag-mini-bioasq\", \"question-answer-passages\", split=\"test\")\n",
    "\n",
    "print(f\"✓ Loaded {len(doc_ds):,} documents, {len(query_ds):,} queries\")\n",
    "\n",
    "# Precompute\n",
    "queries = np.array(query_ds['question'])\n",
    "qrels = [np.array(eval(gold)) for gold in query_ds['relevant_passage_ids']]\n",
    "qrels_counts = [len(s) for s in qrels]\n",
    "\n",
    "disable_caching()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BioASQ dataset...\n",
      "✓ Loaded 28,001 documents, 4,719 queries\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "helper_functions",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T05:13:36.622984Z",
     "start_time": "2025-11-04T05:13:36.619568Z"
    }
   },
   "source": [
    "def deduplicate_retrieved_docs(retrieved_ids_all, k):\n",
    "    \"\"\"\n",
    "    Deduplicate document IDs per query, keeping only first (highest-ranked) occurrence.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_ids_all: (n_queries, n_retrieved) array of document IDs\n",
    "        k: Number of unique documents to keep per query\n",
    "    \n",
    "    Returns:\n",
    "        (n_queries, k) array of unique document IDs\n",
    "    \"\"\"\n",
    "    deduped = []\n",
    "    for query_results in retrieved_ids_all:\n",
    "        seen = set()\n",
    "        unique_docs = []\n",
    "        for doc_id in query_results:\n",
    "            if doc_id not in seen and doc_id != 0:\n",
    "                unique_docs.append(doc_id)\n",
    "                seen.add(doc_id)\n",
    "            if len(unique_docs) == k:\n",
    "                break\n",
    "        # Pad if needed\n",
    "        while len(unique_docs) < k:\n",
    "            unique_docs.append(0)\n",
    "        deduped.append(unique_docs)\n",
    "    return np.array(deduped)\n",
    "\n",
    "\n",
    "def rerank_results(queries, retrieved_passages, retrieved_ids, reranker, batch_size=256):\n",
    "    \"\"\"\n",
    "    Rerank retrieved passages using a reranker model (GPU optimized).\n",
    "    \n",
    "    Args:\n",
    "        queries: List of query strings\n",
    "        retrieved_passages: 2D array of passage texts [n_queries, k]\n",
    "        retrieved_ids: 2D array of passage IDs [n_queries, k]\n",
    "        reranker: Reranker model (CrossEncoder or FlagReranker)\n",
    "        batch_size: Batch size for GPU processing\n",
    "    \n",
    "    Returns:\n",
    "        reranked_ids: 2D array of reranked passage IDs [n_queries, k]\n",
    "    \"\"\"\n",
    "    n_queries = len(queries)\n",
    "    k = retrieved_passages.shape[1]\n",
    "    \n",
    "    # Flatten all query-passage pairs\n",
    "    all_pairs = []\n",
    "    for i in range(n_queries):\n",
    "        query = queries[i]\n",
    "        passages = retrieved_passages[i]\n",
    "        pairs = [[query, passage] for passage in passages]\n",
    "        all_pairs.extend(pairs)\n",
    "    \n",
    "    # Score all pairs\n",
    "    if isinstance(reranker, FlagReranker):\n",
    "        all_scores = reranker.compute_score(all_pairs, batch_size=batch_size, normalize=True)\n",
    "        all_scores = np.array(all_scores)\n",
    "    else:\n",
    "        all_scores = reranker.predict(all_pairs, batch_size=batch_size, show_progress_bar=False)\n",
    "    \n",
    "    # Reshape scores back to [n_queries, k]\n",
    "    scores_2d = all_scores.reshape(n_queries, k)\n",
    "    \n",
    "    # Sort by scores for each query\n",
    "    reranked_ids = np.zeros_like(retrieved_ids)\n",
    "    for i in range(n_queries):\n",
    "        sorted_indices = np.argsort(scores_2d[i])[::-1]\n",
    "        reranked_ids[i] = retrieved_ids[i][sorted_indices]\n",
    "    \n",
    "    return reranked_ids\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tracker\n",
    "tracker = ExperimentTracker('chunking-reranking-bioasq')\n",
    "\n",
    "# Experiment configuration\n",
    "embedder_models = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"BAAI/bge-small-en-v1.5\",\n",
    "    # \"BAAI/bge-base-en-v1.5\",\n",
    "]\n",
    "\n",
    "reranker_models = [\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    # \"cross-encoder/ms-marco-MiniLM-L-12-v2\",\n",
    "    \"BAAI/bge-reranker-base\",\n",
    "]\n",
    "\n",
    "chunkers = ['recursive', 'semantic']\n",
    "chunk_sizes = [64, 128]\n",
    "chunk_overlaps = [0, 50, 100]\n",
    "chunk_thresholds = [0.01, 0.02, 0.05,]  # For semantic only\n",
    "\n",
    "initial_retrieve_k = 100\n",
    "rerank_batch_size = 256\n",
    "faiss_metric = 'IP'\n",
    "\n",
    "print(f\"\"\"\\n{'='*80}\n",
    "EXPERIMENT CONFIGURATION\n",
    "{'='*80}\n",
    "Embedders: {len(embedder_models)}\n",
    "Rerankers: {len(reranker_models)}\n",
    "Chunkers: {chunkers}\n",
    "Chunk sizes: {chunk_sizes}\n",
    "Overlaps: {chunk_overlaps}\n",
    "Semantic thresholds: {chunk_thresholds}\n",
    "Initial retrieve: top-{initial_retrieve_k}\n",
    "{'='*80}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Run experiments\n",
    "for embedder_name in embedder_models:\n",
    "    embedder_name_short = embedder_name.split('/')[-1]\n",
    "    \n",
    "    for chunker_type in chunkers:\n",
    "        for chunk_size, chunk_overlap in itertools.product(chunk_sizes, chunk_overlaps):\n",
    "            # Skip invalid combinations\n",
    "            if chunk_size <= chunk_overlap:\n",
    "                continue\n",
    "            \n",
    "            # Determine thresholds to test\n",
    "            thresholds = chunk_thresholds if chunker_type == 'semantic' else [None]\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"Embedder: {embedder_name_short} | Chunker: {chunker_type} | \"\n",
    "                      f\"Size: {chunk_size} | Overlap: {chunk_overlap} | Threshold: {threshold}\")\n",
    "                print(f\"{'='*80}\")\n",
    "                \n",
    "                try:\n",
    "                    # ========== STEP 1: CHUNKING ==========\n",
    "                    print(\"[1/5] Chunking documents...\")\n",
    "                    if chunker_type == 'recursive':\n",
    "                        chunker = RecursiveChunker(\n",
    "                            chunk_size=chunk_size,\n",
    "                            chunk_overlap=chunk_overlap,\n",
    "                            embedder_model=embedder_name,\n",
    "                        )\n",
    "                    else:  # semantic\n",
    "                        chunker = SemanticChunker(\n",
    "                            chunk_size=chunk_size,\n",
    "                            chunk_overlap=chunk_overlap,\n",
    "                            embedder_model=embedder_name,\n",
    "                            threshold=threshold,\n",
    "                        )\n",
    "                    \n",
    "                    chunked_ds = chunker.chunk_dataset(doc_ds, text_col='passage', id_col='id')\n",
    "                    chunked_ds = chunked_ds.rename_column('doc_id', 'parent_id')\n",
    "                    chunked_ds = chunked_ds.rename_column('text', 'passage')\n",
    "                    print(f\"  ✓ Created {len(chunked_ds):,} chunks from {len(doc_ds):,} documents\")\n",
    "                    \n",
    "                    # ========== STEP 2: EMBEDDING ==========\n",
    "                    print(\"[2/5] Embedding...\")\n",
    "                    embedder = LocalEmbedder(embedder_name, device=\"cuda\")\n",
    "                    \n",
    "                    embed_start = time()\n",
    "                    chunked_ds = embed_dataset(chunked_ds, embedder, column=\"passage\")\n",
    "                    query_ds_embedded = embed_dataset(query_ds, embedder, column=\"question\")\n",
    "                    embed_time = time() - embed_start\n",
    "                    print(f\"  ✓ Embedded in {embed_time:.1f}s\")\n",
    "                    \n",
    "                    # ========== STEP 3: FAISS RETRIEVAL ==========\n",
    "                    print(f\"[3/5] Retrieving top-{initial_retrieve_k}...\")\n",
    "                    chunked_ds.add_faiss_index(\n",
    "                        column='embedding',\n",
    "                        string_factory='Flat',\n",
    "                        metric_type=faiss.METRIC_INNER_PRODUCT,\n",
    "                        batch_size=128,\n",
    "                    )\n",
    "                    \n",
    "                    res = chunked_ds.get_index('embedding').search_batch(\n",
    "                        np.array(query_ds_embedded['embedding']),\n",
    "                        k=initial_retrieve_k\n",
    "                    )\n",
    "                    \n",
    "                    # Map chunks to parent docs\n",
    "                    index_to_parent_id = np.array(chunked_ds['parent_id'])\n",
    "                    chunk_passages = np.array(chunked_ds['passage'])\n",
    "                    \n",
    "                    retrieved_chunk_ids = res.total_indices\n",
    "                    retrieved_parent_ids = index_to_parent_id[retrieved_chunk_ids]\n",
    "                    retrieved_passages = chunk_passages[retrieved_chunk_ids]\n",
    "                    print(f\"  ✓ Retrieved {initial_retrieve_k} chunks per query\")\n",
    "                    \n",
    "                    # Test each reranker\n",
    "                    for reranker_name in reranker_models:\n",
    "                        print(f\"\\n  [4/5] Reranking with {reranker_name.split('/')[-1]}...\")\n",
    "                        \n",
    "                        try:\n",
    "                            # Load reranker\n",
    "                            if \"BAAI\" in reranker_name or \"bge-reranker\" in reranker_name:\n",
    "                                reranker = FlagReranker(reranker_name, use_fp16=True)\n",
    "                            else:\n",
    "                                reranker = CrossEncoder(reranker_name, device=\"cuda\")\n",
    "                            \n",
    "                            # Rerank\n",
    "                            rerank_start = time()\n",
    "                            reranked_parent_ids = rerank_results(\n",
    "                                queries, retrieved_passages, retrieved_parent_ids, \n",
    "                                reranker, batch_size=rerank_batch_size\n",
    "                            )\n",
    "                            rerank_time = time() - rerank_start\n",
    "                            \n",
    "                            # Deduplicate\n",
    "                            reranked_parent_ids_dedup = deduplicate_retrieved_docs(\n",
    "                                reranked_parent_ids, max_k=initial_retrieve_k\n",
    "                            )\n",
    "                            print(f\"    ✓ Reranked + deduplicated in {rerank_time:.1f}s\")\n",
    "                            \n",
    "                            # ========== STEP 5: EVALUATION ==========\n",
    "                            print(\"  [5/5] Calculating metrics...\")\n",
    "                            metrics = {}\n",
    "                            for k in [1, 3, 5, 10]:\n",
    "                                reranked_top_k = reranked_parent_ids_dedup[:, :k]\n",
    "                                metrics = {\n",
    "                                    **metrics,\n",
    "                                    **get_metrics(reranked_top_k, query_ds, k),\n",
    "                                }\n",
    "                            \n",
    "                            total_time = embed_time + rerank_time\n",
    "                            metrics = {\n",
    "                                **{k: round(v, 4) for k, v in metrics.items()},\n",
    "                                \"embed_time\": round(embed_time, 1),\n",
    "                                \"rerank_time\": round(rerank_time, 1),\n",
    "                                \"total_time\": round(total_time, 1),\n",
    "                                \"num_chunks\": len(chunked_ds),\n",
    "                            }\n",
    "                            \n",
    "                            # Log to MLflow\n",
    "                            params = {\n",
    "                                'embedder': embedder_name,\n",
    "                                'reranker': reranker_name,\n",
    "                                'chunker': chunker_type,\n",
    "                                'chunk_size': chunk_size,\n",
    "                                'chunk_overlap': chunk_overlap,\n",
    "                                'chunk_threshold': threshold if threshold else 'none',\n",
    "                                'faiss_metric': faiss_metric,\n",
    "                                'initial_k': initial_retrieve_k,\n",
    "                            }\n",
    "                            \n",
    "                            run_name = (\n",
    "                                f\"{embedder_name_short}_{chunker_type}_\"\n",
    "                                f\"cs{chunk_size}_ov{chunk_overlap}_\"\n",
    "                                f\"thr{threshold if threshold else 'none'}_\"\n",
    "                                f\"{reranker_name.split('/')[-1]}\"\n",
    "                            )\n",
    "                            \n",
    "                            tags = {\n",
    "                                'experiment_type': 'chunking+reranking',\n",
    "                                'phase': 'exploration',\n",
    "                                'dataset': 'bioasq-mini',\n",
    "                                'embedder': embedder_name_short,\n",
    "                                'reranker': reranker_name.split('/')[-1],\n",
    "                                'chunker': chunker_type,\n",
    "                            }\n",
    "                            \n",
    "                            with tracker.start_run(run_name=run_name, tags=tags):\n",
    "                                tracker.log_params(params)\n",
    "                                tracker.log_metrics(metrics)\n",
    "                            \n",
    "                            print(f\"\\n  RESULTS:\")\n",
    "                            print(f\"    P@10:    {metrics['P@10']:.4f}\")\n",
    "                            print(f\"    R@10:    {metrics['R@10']:.4f}\")\n",
    "                            print(f\"    MRR@10:  {metrics['MRR@10']:.4f}\")\n",
    "                            print(f\"    nDCG@10: {metrics['nDCG@10']:.4f}\")\n",
    "                            print(f\"    Time:    {total_time:.1f}s\")\n",
    "                            \n",
    "                            # Cleanup reranker\n",
    "                            del reranker\n",
    "                            gc.collect()\n",
    "                            torch.cuda.empty_cache()\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"    ✗ Reranker failed: {e}\")\n",
    "                            if 'reranker' in locals():\n",
    "                                del reranker\n",
    "                            gc.collect()\n",
    "                            torch.cuda.empty_cache()\n",
    "                            continue\n",
    "                    \n",
    "                    # Cleanup\n",
    "                    chunked_ds.drop_index('embedding')\n",
    "                    del embedder\n",
    "                    del chunked_ds\n",
    "                    del query_ds_embedded\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"\\n✗ FAILED: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    continue\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ ALL EXPERIMENTS COMPLETED!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "View all results in MLflow UI at: http://localhost:5000\n",
    "\n",
    "### Key Questions:\n",
    "1. Does semantic chunking outperform recursive chunking with reranking?\n",
    "2. What is the optimal chunk size when reranking is applied?\n",
    "3. Does semantic clustering threshold affect reranking performance?\n",
    "4. Which embedder + chunker + reranker combination works best?\n",
    "5. How much does reranking improve over retrieval-only (compare with notebook 15_2)?\n",
    "\n",
    "### Expected Insights:\n",
    "- Reranking should improve MRR@10 significantly (better at finding the best match)\n",
    "- Smaller chunks might benefit more from reranking (more candidates to reorder)\n",
    "- Semantic chunking might reduce reranking time (fewer chunks = fewer pairs to score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
